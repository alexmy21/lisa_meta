{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.activate(\".\")\n",
    "# Pkg.add(\"WordTokenizers\")\n",
    "# Pkg.add(\"Languages\")\n",
    "# Pkg.instantiate()\n",
    "# Pkg.add(\"SQLite\")\n",
    "# Pkg.add(\"DBInterface\")\n",
    "# Pkg.add(\"JSON3\")\n",
    "# Pkg.add(\"EasyConfig\")\n",
    "# Pkg.add(\"MurmurHash3\")\n",
    "# Pkg.add(\"TextAnalysis\")\n",
    "# Pkg.add(\"UUIDs\")\n",
    "# Pkg.add(\"PooledArrays\")\n",
    "include(\"src/lisa_store.jl\")\n",
    "\n",
    "using SQLite\n",
    "using DBInterface\n",
    "using MurmurHash3\n",
    "using TextAnalysis\n",
    "using JSON3\n",
    "using PooledArrays\n",
    "using UUIDs\n",
    "using EasyConfig\n",
    "using HDF5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domains and co-domains\n",
    "\n",
    "Here we'll try to reduce ambiguity of tracing tokens from the HllSet presentation.  \n",
    "\n",
    "The consistency of conversion of datasets to HllSets depends on two factors:\n",
    "\n",
    " 1. The $p$ parameter that defines the precision of the conversion through the number of bins;\n",
    " 2. The type of a hash function that we are using.\n",
    "\n",
    "Specifically, the output from hash function depends on the seed values used in initiating of the hash function. Applying different seed values we can control the generated hashes.\n",
    "\n",
    "Here is an idea of the algorithm:\n",
    "\n",
    " 1. We are performing the dataset processing as usual, utilizing standard hash function:\n",
    "\n",
    "$$F_{(std)}: X_{(std)} \\to Y_{(std)}$$\n",
    "\n",
    " 2. Then we are tracing original tokens by applying back processing:\n",
    "\n",
    "$$G_{(std)}: Y_{(std)} \\to X_{(std)}$$\n",
    "\n",
    " 3. Now we can perform the same dataset processing using the same hash function but with different seed values:\n",
    "\n",
    "$$F_{(seed)}: X_{(seed)} \\to Y_{(seed)}$$\n",
    "\n",
    " 4. And we will trace back the results from modified hash function:\n",
    "\n",
    "$$G_{(seed)}: Y_{(seed)} \\to X_{(seed)}$$\n",
    "\n",
    "It is possible, that\n",
    "\n",
    " $$X_{(seed)} \\not= X_{(std)}$$\n",
    "\n",
    "But it is also obvious, that tokens from the original dataset should be in both results.\n",
    "\n",
    "## Standard processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Graph.DB(\"lisa_analytics.db\")\n",
    "db.sqlitedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store.book_file(db, \"/home/alexmy/JULIA/DEMO/sample/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = string(uuid4())\n",
    "df = Graph.set_lock!(db, \n",
    "    \"/home/alexmy/JULIA/DEMO/sample\", \n",
    "    \"csv\", \n",
    "    \"book_file\", \n",
    "    \"ingest_csv\", \n",
    "    \"waiting\", \n",
    "    \"waiting\", \n",
    "    uuid; result=true)\n",
    "\n",
    "for row in eachrow(df)\n",
    "    assign = Graph.Assignment(row) \n",
    "    col_uuid = string(uuid4())\n",
    "    Store.ingest_csv_by_column(db, assign, col_uuid; limit=10000, offset=10)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_id = \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\"\n",
    "node = Graph.getnode(db, ds_id, :; table_name=\"t_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Graph.gettokens(db, \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\", :)\n",
    "tokens = Store.collect_tokens(db, result)\n",
    "println(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing with seeded hash\n",
    "\n",
    "We'll go through the same steps with the same params except the database.\n",
    " - the db name would be \"db_seed.db\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_seed = Graph.DB(\"db_seed.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store.book_file(db_seed, \"/home/alexmy/JULIA/DEMO/sample/\"; seed=42, P=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = string(uuid4())\n",
    "df = Graph.set_lock!(db_seed, \n",
    "    \"/home/alexmy/JULIA/DEMO/sample\", \n",
    "    \"csv\", \n",
    "    \"book_file\", \n",
    "    \"ingest_csv\", \n",
    "    \"waiting\", \n",
    "    \"waiting\", \n",
    "    uuid; result=true)\n",
    "\n",
    "for row in eachrow(df)\n",
    "    assign = Graph.Assignment(row) \n",
    "    col_uuid = string(uuid4())\n",
    "    # Important, do not forget to set HllSet precission parameter p to 8\n",
    "    Store.ingest_csv_by_column(db_seed, assign, col_uuid; limit=10000, offset=10, p=10, seed=42)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are getting the dataset directly from **nodes** table of the \"db_seed.db\" database.\n",
    "\n",
    "We are utilizing the fact that SHA1 node ID is not affected by changing the hash function for the tokens encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_id = \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\"\n",
    "node_seed = Graph.getnode(db_seed, ds_id, :; table_name=\"t_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Graph.gettokens(db, \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\", :)\n",
    "tokens_seed = Store.collect_tokens(db_seed, result)\n",
    "println(tokens_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = intersect(tokens, tokens_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"tokens size: \", length(tokens), \n",
    "    \";\\ntokens_seed size: \", length(tokens_seed), \n",
    "    \";\\nintersection size: \", length(intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check how use of a seeded hash affected HllSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hll_std = SetCore.HllSet{10}()\n",
    "hll_seed = SetCore.HllSet{10}()\n",
    "\n",
    "dataset_std = node.dataset\n",
    "dataset_seed = node_seed.dataset\n",
    "\n",
    "println(\"dataset_std size: \", length(dataset_std), \n",
    "    \";\\ndataset_seed size: \", length(dataset_seed))\n",
    "\n",
    "# Restore collect_hll_sets\n",
    "hll_std = SetCore.restore(hll_std, Vector{UInt64}(dataset_std))\n",
    "hll_seed = SetCore.restore(hll_seed, Vector{UInt64}(dataset_seed))\n",
    "\n",
    "println(\"hll_std size: \", SetCore.count(hll_std), \n",
    "    \";\\nhll_seed size: \", SetCore.count(hll_seed))\n",
    "\n",
    "hll_intersection = intersect(hll_std, hll_seed)\n",
    "# SetCore.count(hll_intersection)\n",
    "\n",
    "println(\"hll_intersection size: \", SetCore.count(hll_intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we are lucky , we got not empty intersection from two HllSets built using different hash functions. (Or may be not, because $1$ is small and could be within the range of an estimation error)\n",
    "\n",
    "We also can see that the cardinality estimations in our case are not bad. The difference in both case is equal $2$, or about $2.63$%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying HllSets for Tabular data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"src/lisa_store.jl\")\n",
    "\n",
    "using SQLite\n",
    "using DBInterface\n",
    "using MurmurHash3\n",
    "using TextAnalysis\n",
    "using JSON3\n",
    "using PooledArrays\n",
    "using UUIDs\n",
    "using EasyConfig\n",
    "using SparseArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Graph.DB(\"lisa_analytics.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store.book_file(db, \"/home/alexmy/JULIA/DEMO/sample/\"; column=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = string(uuid4())\n",
    "df = Graph.set_lock!(db, \n",
    "    \"/home/alexmy/JULIA/DEMO/sample\", \n",
    "    \"csv\", \n",
    "    \"book_file\", \n",
    "    \"ingest_csv\", \n",
    "    \"waiting\", \n",
    "    \"waiting\", \n",
    "    uuid; result=true)\n",
    "\n",
    "for row in eachrow(df)\n",
    "    assign = Graph.Assignment(row) \n",
    "    col_uuid = string(uuid4())    \n",
    "    Store.ingest_csv_by_row(db, assign; limit=50, offset=10)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide csv file sha1 id to extract row and column nodes\n",
    "source_id = \"0b90b1fee69c77ffa3efe57db7788112ef96dba6\"\n",
    "\"\"\"\n",
    "    Here we are going to extract row and column nodes from the csv file.\n",
    "    The resulting matrix will show the cardinality of intersection of row and column nodes.\n",
    "\"\"\"\n",
    "matrix = Store.get_card_matrix(db, source_id)\n",
    "\n",
    "for row in eachrow(matrix)\n",
    "    println(row)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Running get_node_matrix function to extract row and column nodes from the csv file.\n",
    "    Each cell of the resulting matrix will hold a node that would represent an intersection\n",
    "    of corresponding row and column of the original csv file.\n",
    "\"\"\"\n",
    "node_matrix = Store.get_node_matrix(db, source_id)\n",
    "\n",
    "for row in eachrow(node_matrix)\n",
    "    println(row)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Finally we are going to recreate the original csv file (a sample in our case) \n",
    "    from the node matrix.\n",
    "\n",
    "    Important to keep in mind that the results of each cell in the matrix would not be the same \n",
    "    as in the original csv file.\n",
    "    Possible discrepancies can include wrong order on tokens in multitoken cells, \n",
    "    missing cells, etc.\n",
    "    It is a natural result of the probalistic approximation performed on original csv file.\n",
    "    The original csv file was tokenized, compacted into HllSet, and then reconstructed.\n",
    "\"\"\"\n",
    "value_matrix = Store.get_value_matrix(db, source_id)\n",
    "\n",
    "for row in eachrow(value_matrix)\n",
    "    println(row)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
