{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.activate(\".\")\n",
    "# Pkg.add(\"WordTokenizers\")\n",
    "# Pkg.add(\"Languages\")\n",
    "# Pkg.instantiate()\n",
    "# Pkg.add(\"SQLite\")\n",
    "# Pkg.add(\"DBInterface\")\n",
    "# Pkg.add(\"JSON3\")\n",
    "# Pkg.add(\"EasyConfig\")\n",
    "# Pkg.add(\"MurmurHash3\")\n",
    "# Pkg.add(\"TextAnalysis\")\n",
    "# Pkg.add(\"UUIDs\")\n",
    "# Pkg.add(\"PooledArrays\")\n",
    "include(\"src/lisa_store.jl\")\n",
    "\n",
    "using SQLite\n",
    "using DBInterface\n",
    "using MurmurHash3\n",
    "using TextAnalysis\n",
    "using JSON3\n",
    "using PooledArrays\n",
    "using UUIDs\n",
    "using EasyConfig\n",
    "using HDF5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domains and co-domains\n",
    "\n",
    "Here we'll try to reduce ambiguity of tracing tokens from the HllSet presentation.  \n",
    "\n",
    "The consistency of conversion of datasets to HllSets depends on two factors:\n",
    "\n",
    " 1. The $p$ parameter that defines the precision of the conversion through the number of bins;\n",
    " 2. The type of a hash function that we are using.\n",
    "\n",
    "Specifically, the output from hash function depends on the seed values used in initiating of the hash function. Applying different seed values we can control the generated hashes.\n",
    "\n",
    "Here is an idea of the algorithm:\n",
    "\n",
    " 1. We are performing the dataset processing as usual, utilizing standard hash function:\n",
    "\n",
    "$$F_{(std)}: X_{(std)} \\to Y_{(std)}$$\n",
    "\n",
    " 2. Then we are tracing original tokens by applying back processing:\n",
    "\n",
    "$$G_{(std)}: Y_{(std)} \\to X_{(std)}$$\n",
    "\n",
    " 3. Now we can perform the same dataset processing using the same hash function but with different seed values:\n",
    "\n",
    "$$F_{(seed)}: X_{(seed)} \\to Y_{(seed)}$$\n",
    "\n",
    " 4. And we will trace back the results from modified hash function:\n",
    "\n",
    "$$G_{(seed)}: Y_{(seed)} \\to X_{(seed)}$$\n",
    "\n",
    "It is possible, that\n",
    "\n",
    " $$X_{(seed)} \\not= X_{(std)}$$\n",
    "\n",
    "But it is also obvious, that tokens from the original dataset should be in both results.\n",
    "\n",
    "## Standard processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQLite.DB(\"lisa_analytics.db\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db = Graph.DB(\"lisa_analytics.db\")\n",
    "db.sqlitedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store.book_file(db, \"/home/alexmy/JULIA/DEMO/sample/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed column: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed column: 15\n"
     ]
    }
   ],
   "source": [
    "uuid = string(uuid4())\n",
    "df = Graph.set_lock!(db, \n",
    "    \"/home/alexmy/JULIA/DEMO/sample\", \n",
    "    \"csv\", \n",
    "    \"book_file\", \n",
    "    \"ingest_csv\", \n",
    "    \"waiting\", \n",
    "    \"waiting\", \n",
    "    uuid; result=true)\n",
    "\n",
    "for row in eachrow(df)\n",
    "    assign = Graph.Assignment(row) \n",
    "    col_uuid = string(uuid4())\n",
    "    Store._ingest_csv_by_column(db, assign, col_uuid; limit=10000, offset=10)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(3f9526f8d331b9519b8632a11b2d344ab7c647b6; [\"csv_column\"]; props: column_name=\"Vehicle type\", file_sha1=\"0b90b1fee69c77ffa3efe57db7788112ef96dba6\", column_type=\"String\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_id = \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\"\n",
    "node = Graph.getnode(db, ds_id, :; table_name=\"t_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set([\"Taxi\", \"mgw\", \"Taxi/Private\", \"Moped\", \"String\", \"M/cycle\", \"vehicle\", \"and\", \"seats\", \"over\", \"coach\", \"more\", \"Motor\", \"pass\", \"Minibus\", \"horse\", \"motor\", \"under\", \"hire\", \"cycle\", \"Motorcycle\", \"Private\", \"Vehicle\", \"Other\", \"Goods\", \"tonnes\", \"Bus\", \"gross\", \"Van\", \"Ridden\", \"Car\", \"weight\", \"car\", \"passenger\", \"from\", \"type\", \"maximum\", \"Pedal\"])\n"
     ]
    }
   ],
   "source": [
    "tokens = Store.collect_tokens(db, \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\")\n",
    "println(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing with seeded hash\n",
    "\n",
    "We'll go through the same steps with the same params except the database.\n",
    " - the db name would be \"db_seed.db\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph.DB(\"db_seed.db\") (34 assignments, 0 commits, 166 tokens, 0 nodes, 0 edges25 t_nodes, 23 t_edges)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_seed = Graph.DB(\"db_seed.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store.book_file(db_seed, \"/home/alexmy/JULIA/DEMO/sample/\"; seed=42, P=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed column: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed column: 15\n"
     ]
    }
   ],
   "source": [
    "uuid = string(uuid4())\n",
    "df = Graph.set_lock!(db_seed, \n",
    "    \"/home/alexmy/JULIA/DEMO/sample\", \n",
    "    \"csv\", \n",
    "    \"book_file\", \n",
    "    \"ingest_csv\", \n",
    "    \"waiting\", \n",
    "    \"waiting\", \n",
    "    uuid; result=true)\n",
    "\n",
    "for row in eachrow(df)\n",
    "    assign = Graph.Assignment(row) \n",
    "    col_uuid = string(uuid4())\n",
    "    # Important, do not forget to set HllSet precission parameter p to 8\n",
    "    Store._ingest_csv_by_column(db_seed, assign, col_uuid; limit=10000, offset=10, p=10, seed=42)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are getting the dataset directly from **nodes** table of the \"db_seed.db\" database.\n",
    "\n",
    "We are utilizing the fact that SHA1 node ID is not affected by changing the hash function for the tokens encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(3f9526f8d331b9519b8632a11b2d344ab7c647b6; [\"csv_column\"]; props: column_name=\"Vehicle type\", file_sha1=\"0b90b1fee69c77ffa3efe57db7788112ef96dba6\", column_type=\"String\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_id = \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\"\n",
    "node_seed = Graph.getnode(db_seed, ds_id, :; table_name=\"t_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set([\"maximum\", \"Pedal\", \"Taxi/Private\", \"Moped\", \"M/cycle\", \"String\", \"over\", \"and\", \"seats\", \"vehicle\", \"coach\", \"more\", \"Minibus\", \"pass\", \"Motor\", \"horse\", \"hire\", \"motor\", \"Vehicle\", \"cycle\", \"Goods\", \"Motorcycle\", \"Private\", \"Other\", \"under\", \"tonnes\", \"Bus\", \"gross\", \"Van\", \"Ridden\", \"Car\", \"weight\", \"car\", \"passenger\", \"from\", \"type\", \"mgw\", \"Taxi\"])\n"
     ]
    }
   ],
   "source": [
    "tokens_seed = Store.collect_tokens(db_seed, \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\")\n",
    "println(tokens_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set{String} with 38 elements:\n",
       "  \"mgw\"\n",
       "  \"Taxi\"\n",
       "  \"Taxi/Private\"\n",
       "  \"Moped\"\n",
       "  \"M/cycle\"\n",
       "  \"String\"\n",
       "  \"vehicle\"\n",
       "  \"and\"\n",
       "  \"seats\"\n",
       "  \"over\"\n",
       "  \"coach\"\n",
       "  \"more\"\n",
       "  \"Minibus\"\n",
       "  \"pass\"\n",
       "  \"Motor\"\n",
       "  \"horse\"\n",
       "  \"hire\"\n",
       "  \"motor\"\n",
       "  \"Vehicle\"\n",
       "  â‹® "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intersection = intersect(tokens, tokens_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens size: 38;\n",
      "tokens_seed size: 38;\n",
      "intersection size: 38\n"
     ]
    }
   ],
   "source": [
    "println(\"tokens size: \", length(tokens), \n",
    "    \";\\ntokens_seed size: \", length(tokens_seed), \n",
    "    \";\\nintersection size: \", length(intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check how use of a seeded hash affected HllSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_std size: 1024;\n",
      "dataset_seed size: 1024\n",
      "hll_std size: 36;\n",
      "hll_seed size: 36\n",
      "hll_intersection size: 1\n"
     ]
    }
   ],
   "source": [
    "hll_std = SetCore.HllSet{10}()\n",
    "hll_seed = SetCore.HllSet{10}()\n",
    "\n",
    "dataset_std = node.dataset\n",
    "dataset_seed = node_seed.dataset\n",
    "\n",
    "println(\"dataset_std size: \", length(dataset_std), \n",
    "    \";\\ndataset_seed size: \", length(dataset_seed))\n",
    "\n",
    "# Restore collect_hll_sets\n",
    "hll_std = SetCore.restore(hll_std, Vector{UInt64}(dataset_std))\n",
    "hll_seed = SetCore.restore(hll_seed, Vector{UInt64}(dataset_seed))\n",
    "\n",
    "println(\"hll_std size: \", SetCore.count(hll_std), \n",
    "    \";\\nhll_seed size: \", SetCore.count(hll_seed))\n",
    "\n",
    "hll_intersection = intersect(hll_std, hll_seed)\n",
    "# SetCore.count(hll_intersection)\n",
    "\n",
    "println(\"hll_intersection size: \", SetCore.count(hll_intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we are lucky , we got not empty intersection from two HllSets built using different hash functions. (Or may be not, because $1$ is small and could be within the range of an estimation error)\n",
    "\n",
    "We also can see that the cardinality estimations in our case are not bad. The difference in both case is equal $2$, or about $2.63$%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying HllSets for Tabular data structures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
