{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.activate(\".\")\n",
    "# Pkg.instantiate()\n",
    "# Pkg.add(\"SQLite\")\n",
    "# Pkg.add(\"DBInterface\")\n",
    "# Pkg.add(\"JSON3\")\n",
    "# Pkg.add(\"EasyConfig\")\n",
    "# Pkg.add(\"MurmurHash3\")\n",
    "# Pkg.add(\"TextAnalysis\")\n",
    "# Pkg.add(\"UUIDs\")\n",
    "# Pkg.add(\"PooledArrays\")\n",
    "include(\"src/lisa_store.jl\")\n",
    "\n",
    "using SQLite\n",
    "using DBInterface\n",
    "using MurmurHash3\n",
    "using TextAnalysis\n",
    "using JSON3\n",
    "using PooledArrays\n",
    "using UUIDs\n",
    "using EasyConfig\n",
    "using HDF5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domains and co-domains\n",
    "\n",
    "Here we'll try to reduce ambiguity of tracing tokens from the HllSet presentation.  \n",
    "\n",
    "The consistency of conversion of datasets to HllSets depends on two factors:\n",
    "\n",
    " 1. The $p$ parameter that defines the precision of the conversion through the number of bins;\n",
    " 2. The type of a hash function that we are using.\n",
    "\n",
    "Specifically, the output from hash function depends on the seed values used in initiating of the hash function. Applying different seed values we can control the generated hashes.\n",
    "\n",
    "Here is an idea of the algorithm:\n",
    "\n",
    " 1. We are performing the dataset processing as usual, utilizing standard hash function:\n",
    "\n",
    "$$F_{(std)}: X_{(std)} \\to Y_{(std)}$$\n",
    "\n",
    " 2. Then we are tracing original tokens by applying back processing:\n",
    "\n",
    "$$G_{(std)}: Y_{(std)} \\to X_{(std)}$$\n",
    "\n",
    " 3. Now we can perform the same dataset processing using the same hash function but with different seed values:\n",
    "\n",
    "$$F_{(seed)}: X_{(seed)} \\to Y_{(seed)}$$\n",
    "\n",
    " 4. And we will trace back the results from modified hash function:\n",
    "\n",
    "$$G_{(seed)}: Y_{(seed)} \\to X_{(seed)}$$\n",
    "\n",
    "It is obvious, that\n",
    "\n",
    " $$X_{(seed)} \\not= X_{(std)}$$\n",
    "\n",
    "But it is also obvious, that tokens from the original dataset should be in both results.\n",
    "\n",
    "## Standard processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"lisa_analytics.hdf5\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db = Graph.DB(\"lisa_analytics.db\")\n",
    "hll = SetCore.HllSet{10}()\n",
    "\n",
    "# Create an empty HDF5 file. Run it only once, because it will overwrite the file\n",
    "# h5open(\"lisa_analytics.hdf5\", \"w\") do f\n",
    "#     # The file is now open, but it's empty\n",
    "# end\n",
    "\n",
    "hdf5_file_name = \"lisa_analytics.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10\n",
    "hll = SetCore.HllSet{p}()\n",
    "Store.book_file(db, \"/home/alexmy/JULIA/DEMO/sample/\", hll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = string(uuid4())\n",
    "df = Graph.set_lock!(db, \n",
    "    \"/home/alexmy/JULIA/DEMO/sample\", \n",
    "    \"csv\", \n",
    "    \"book_file\", \n",
    "    \"ingest_csv\", \n",
    "    \"waiting\", \n",
    "    \"waiting\", \n",
    "    uuid; result=true)\n",
    "\n",
    "for row in eachrow(df)\n",
    "    assign = Graph.Assignment(row) \n",
    "    col_uuid = string(uuid4())\n",
    "    Store._ingest_csv_by_column(db, assign, col_uuid; limit=10000, offset=10)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store.commit(db, hdf5_file_name, \"Alex Mylnikov\", \"alexmy@lisa-park.com\", \"commit 1\", Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = h5open(hdf5_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close(analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "commit_id = \"fa385668-f538-4771-bbca-3464c861620b\"\n",
    "ds_id = \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\"\n",
    "dataset_path = \"/$commit_id/nodes/$ds_id/_csv_column_\"\n",
    "data_out = Dict()\n",
    "h5open(hdf5_file_name, \"r\") do file\n",
    "    Store.read_datasets(file, data_out, dataset_path)\n",
    "end\n",
    "println(data_out[\"_csv_column_\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set([\"Taxi/Private\", \"vehicle\", \"seats\", \"coach\", \"more\", \"Motor\", \"horse\", \"motor\", \"cycle\", \"Motorcycle\", \"Private\", \"gross\", \"weight\", \"from\", \"mgw\", \"maximum\", \"Pedal\"])\n"
     ]
    }
   ],
   "source": [
    "dataset = data_out[\"_csv_column_\"]\n",
    "tokens = Store.collect_tokens(dataset, ds_id, db)\n",
    "println(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing with seeded hash\n",
    "\n",
    "We'll go through the same steps with the same params except the database and HDF5 file.\n",
    " - the db name would be \"db_seed.db\"\n",
    " - the HDF5 name would be \"hdf5_seed.hdf5\" (only, if we want to use commit)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HllSet{10}()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_seed = Graph.DB(\"db_seed.db\")\n",
    "hll = SetCore.HllSet{10}()\n",
    "\n",
    "# Create an empty HDF5 file. Run it only once, because it will overwrite the file\n",
    "# h5open(\"hdf5_seed.hdf5\", \"w\") do f\n",
    "#     # The file is now open, but it's empty\n",
    "# end\n",
    "\n",
    "# hdf5_seed = \"hdf5_seed.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Store.book_file(db_seed, \"/home/alexmy/JULIA/DEMO/sample/\", hll; seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = string(uuid4())\n",
    "df = Graph.set_lock!(db_seed, \n",
    "    \"/home/alexmy/JULIA/DEMO/sample\", \n",
    "    \"csv\", \n",
    "    \"book_file\", \n",
    "    \"ingest_csv\", \n",
    "    \"waiting\", \n",
    "    \"waiting\", \n",
    "    uuid; result=true)\n",
    "\n",
    "for row in eachrow(df)\n",
    "    assign = Graph.Assignment(row) \n",
    "    col_uuid = string(uuid4())\n",
    "    Store._ingest_csv_by_column(db_seed, assign, col_uuid; limit=10000, offset=10, seed=42)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(3f9526f8d331b9519b8632a11b2d344ab7c647b6, \"csv_column\"; column_name=\"Vehicle type\", file_sha1=\"0b90b1fee69c77ffa3efe57db7788112ef96dba6\", column_type=\"String\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_id = \"3f9526f8d331b9519b8632a11b2d344ab7c647b6\"\n",
    "node = Graph.getnode(db_seed, ds_id, :; table_name=\"t_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set([\"Taxi/Private\", \"vehicle\", \"seats\", \"coach\", \"more\", \"Motor\", \"horse\", \"motor\", \"cycle\", \"Motorcycle\", \"Private\", \"gross\", \"weight\", \"from\", \"mgw\", \"maximum\", \"Pedal\"])\n"
     ]
    }
   ],
   "source": [
    "tokens_seed = Store.collect_tokens(node.dataset, ds_id, db_seed)\n",
    "println(tokens_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set{String} with 17 elements:\n",
       "  \"Taxi/Private\"\n",
       "  \"vehicle\"\n",
       "  \"seats\"\n",
       "  \"coach\"\n",
       "  \"more\"\n",
       "  \"Motor\"\n",
       "  \"horse\"\n",
       "  \"motor\"\n",
       "  \"cycle\"\n",
       "  \"Motorcycle\"\n",
       "  \"Private\"\n",
       "  \"gross\"\n",
       "  \"weight\"\n",
       "  \"from\"\n",
       "  \"mgw\"\n",
       "  \"maximum\"\n",
       "  \"Pedal\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intersection = intersect(tokens, tokens_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens size: 17;\n",
      "tokens_seed size: 17;\n",
      "intersection size: 17\n"
     ]
    }
   ],
   "source": [
    "println(\"tokens size: \", length(tokens), \n",
    "    \";\\ntokens_seed size: \", length(tokens_seed), \n",
    "    \";\\nintersection size: \", length(intersection))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
